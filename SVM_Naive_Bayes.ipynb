{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCMLSjSnLTwA"
      },
      "outputs": [],
      "source": [
        "1.   What is a Support Vector Machine (SVM), and how does it work?\n",
        "   -> A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "\n",
        "  #  1. Separating Hyperplane:\n",
        "     *  SVMs aim to find a decision boundary (hyperplane) that effectively divides data points into different classes.\n",
        "     *  In a 2D space, this hyperplane is a line. In higher dimensions, it becomes a plane or a hyperplane.\n",
        "\n",
        "# 2. Support Vectors:\n",
        "    *  Support vectors are the data points closest to the separating hyperplane.\n",
        "    *  They are crucial in defining the margin and the position of the hyperplane.\n",
        "    *  SVMs are less sensitive to other data points that are further away from the margin.\n",
        "\n",
        "# 3. Kernel Trick:\n",
        "     *  SVMs can handle non-linearly separable data by using a kernel function.\n",
        "     *  The kernel function transforms the data into a higher-dimensional space where it might be linearly separable.\n",
        "\n",
        "# 4. Classification and Regression:\n",
        "# Classification:\n",
        "     SVMs classify data points into different categories based on their position relative to the hyperplane.\n",
        "\n",
        "# 5. Key Advantages of SVMs:\n",
        "# Effective in high-dimensional spaces:\n",
        "           They can handle a large number of features effectively.\n",
        "# Memory efficient:\n",
        "           They rely on support vectors, which can make them memory efficient for large datasets.\n",
        "# Versatile:\n",
        " They can be used for both classification and regression tasks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "2.  Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "    #  -> * Hard Margin SVM\n",
        "\n",
        "# Requires Linearly Separable Data:\n",
        "             This method is only applicable when all data points can be perfectly separated by a hyperplane with no overlaps or outliers.\n",
        "# No Misclassifications:\n",
        "             The primary goal is to find a hyperplane that provides the widest possible margin between the classes without any data\n",
        "               points being misclassified or falling within the margin.\n",
        "# Sensitive to Outliers:\n",
        "         Because it demands perfect separation, a single outlier can significantly influence the decision boundary, leading to a poorly generalized model.\n",
        "# Soft Margin SVm\n",
        "\n",
        "# Accommodates Imperfect Data:\n",
        "                This approach is designed for datasets that are not perfectly linearly separable, including those with noise\n",
        "                or overlapping classes.\n",
        "# Uses Slack Variables:\n",
        "              To allow for flexibility, soft margin SVM introduces \"slack variables\" (ξi) that permit some data points to be within\n",
        "              the margin or even on the wrong side of the hyperplane.\n",
        "# Balances Margin and Error:\n",
        "              The model optimizes by finding a hyperplane that balances maximizing the margin with minimizing the penalties for misclassified\n",
        "               points, using a regularization parameter (often denoted as C) to control this trade-off.\n",
        "# More Robust and Generalizable:\n",
        "               By tolerating a small error on the training data, soft margin SVM is less sensitive to outliers and tends to"
      ],
      "metadata": {
        "id": "LU1zBsMJMo4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. : What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "     -> The Kernel Trick in Support Vector Machines (SVMs) is a technique that allows SVMs to find a linear decision boundary in\n",
        "        a higher-dimensional feature space without explicitly transforming the data into that higher dimension. This is particularly\n",
        "        useful when the data is not linearly separable in its original feature space.\n",
        "\n",
        "\n",
        "\n",
        "# Example of a Kernel:\n",
        " Radial Basis Function (RBF) Kernel\n",
        "     The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is a widely used kernel in SVMs. Its formula is given by:\n",
        "\n",
        "K(x, y) = exp(-γ * ||x - y||²)\n",
        "\n",
        "where:\n",
        "\n",
        "x and y are the input data points.\n",
        "||x - y||² is the squared Euclidean distance between x and y.\n",
        "γ (gamma) is a hyperparameter that controls the influence of individual training samples.\n"
      ],
      "metadata": {
        "id": "dyud1CorMpb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. : What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "    -> A Naïve Bayes classifier is a simple, probabilistic machine learning algorithm that uses Bayes' theorem to predict class\n",
        "       membership probabilities for data points.\n",
        "\n",
        "      #  Naïve Bayes Classifier Does\n",
        "# Probabilistic Classification:\n",
        "      It calculates the probability of a given data point belonging to different classes.\n",
        "# Bayes' Theorem:\n",
        "            It's built upon Bayes' theorem, which provides a way to update prior beliefs (prior probability) based on new evidence\n",
        "              to arrive at a revised belief (posterior probability).\n",
        "\n",
        "# Why It's Called \"Naïve\"\n",
        "\n",
        "# Unrealistic Independence Assumption:\n",
        "        The \"naïve\" label comes from the algorithm's fundamental assumption that its features are independent.\n",
        "# Real-World Data is Complex:\n",
        "         In reality, features in most datasets are not truly independent; they often have correlations and dependencies that this algorithm ignores.\n",
        "# Surprising Effectiveness:\n",
        "           Despite being \"naïve,\" the algorithm often performs surprisingly well in practice, particularly in tasks like text classification,\n",
        "             spam filtering, and sentiment analysis. This is because the independence assumption often simplifies calculations without\n",
        "             significantly compromising the overall accuracy for certain types of problems"
      ],
      "metadata": {
        "id": "GwOwG8HUMpsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. : Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "   ->  Gaussian Naive Bayes assumes features follow a normal distribution and is used for continuous data. Multinomial Naive\n",
        "      Bayes works with discrete data, like word counts in text, and is often used in Natural Language Processing tasks.\n",
        "     Bernoulli Naive Bayes is designed for binary or boolean features, such as whether a word appears in a document or not.\n",
        "  #  Gaussian Naive Bayes:\n",
        "Assumption:\n",
        "      Features are continuous and follow a normal (Gaussian) distribution.\n",
        "# Use Cases:\n",
        "       Suitable for datasets with continuous numerical features, like height, weight, temperature, or any measurement where data points tend to cluster around a mean.\n",
        "Exa/mple:\n",
        "        Classifying flower species based on petal length and width, where petal measurements are likely to be normally distributed.\n",
        "\n",
        "# Assumption:\n",
        "         Features are discrete and represent counts or frequencies.\n",
        "Use Cases:\n",
        "           Commonly used in text classification, spam filtering, and document categorization, where word counts or term frequencies are important"
      ],
      "metadata": {
        "id": "CkTUWY7IMpxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.  : Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with a linear kernel\n",
        "# C is the regularization parameter. A smaller C means more regularization.\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Print the support vectors\n",
        "# Support vectors are the data points closest to the hyperplane\n",
        "print(\"\\nSupport Vectors:\")\n",
        "# The support_vectors_ attribute contains the coordinates of the support vectors\n",
        "print(svm_model.support_vectors_)\n",
        "\n",
        "# Print the indices of the support vectors in the training data\n",
        "print(\"\\nIndices of Support Vectors in training data:\")\n",
        "print(svm_model.support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4EiykmTMpzN",
        "outputId": "25388c95-f860-46d8-b1e7-c56f98d3abec"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Indices of Support Vectors in training data:\n",
            "[ 16  18  76   7  30  39  44  45  47  58  64  65  90  95   1  15  27  53\n",
            "  66  72  86  97  98 101]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "7. : Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train Gaussian Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Gaussian Naïve Bayes:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0LFz5utMp2v",
        "outputId": "3b987573-492d-4bac-b326-fa40d8443b5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Gaussian Naïve Bayes:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.93      0.90      0.92        42\n",
            "      benign       0.95      0.96      0.95        72\n",
            "\n",
            "    accuracy                           0.94       114\n",
            "   macro avg       0.94      0.93      0.93       114\n",
            "weighted avg       0.94      0.94      0.94       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "8.: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf'] # Using RBF kernel for gamma parameter\n",
        "}\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Get the best estimator (model with best hyperparameters)\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDOae9OvMp39",
        "outputId": "e468bf81-43eb-401c-f478-791e0d521cf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on Test Set: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "9. 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset (binary classification: e.g. two categories)\n",
        "categories = ['rec.sport.baseball', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "\n",
        "X, y = newsgroups.data, newsgroups.target\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Naïve Bayes Classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgSlDn-6Mp8K",
        "outputId": "3c428bac-dae0-4432-c472-8ec25409655a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "10. : Imagine you’re working as a data scientist for a company that handles email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --> Data Preprocessing:\n",
        "Text Cleaning:\n",
        "Lowercase conversion:\n",
        "         Convert all text to lowercase for consistent comparison.\n",
        "Remove punctuation:\n",
        "        Remove unnecessary punctuation like commas, periods, and exclamation marks to focus on the meaningful words.\n",
        "# Stop word removal:\n",
        "             Eliminate common words with little meaning (e.g., \"the\", \"a\", \"and\") that might noise up the analysis.\n",
        "Stemming or Lemmatization:\n",
        "                   Normalize words to their root form (e.g., \"running\" and \"runs\" become \"run\") to reduce variations and improve feature representation.\n",
        "# Text Vectorization:\n",
        "Bag-of-Words (BoW):\n",
        "             Represent each email as a vector where each element corresponds to a unique word in the vocabulary, with its count indicating how often it appears in the email.\n",
        "# Term Frequency-Inverse Document Frequency (TF-IDF):\n",
        "              Weights words based on their frequency within the email and across the entire corpus, giving more importance to rare but relevant words.\n",
        "N-grams:\n",
        "            Consider sequences of n words (e.g., bigrams, trigrams) to capture phrases and context.\n",
        "# Handling Missing Data:\n",
        "# Imputation\n",
        "           : Fill missing values with a predefined value (e.g., \"unknown\" for text fields, mean or median for numerical values).\n",
        "# Dropping Missing Values:\n",
        "                    If the percentage of missing values is high in a specific feature, consider dropping that feature or imputing based on other relevant features.\n",
        "\n",
        "\n",
        "                    # Model Selection (SVM vs. Naïve Bayes):\n",
        "\n",
        "\n",
        "Naïve Bayes:\n",
        "Advantages:\n",
        "                Simple, efficient, performs well with large datasets and sparse features, good for text classification due to its assumption\n",
        "                  of independence between words (which is not entirely accurate but often works well in practice).\n",
        "Disadvantages:\n",
        "      Can be sensitive to highly skewed class distributions.\n",
        "Support Vector Machines (SVM):\n",
        "Advantages:\n",
        "        Effective with non-linear data, good at handling class imbalances, can handle high-dimensional data.\n",
        "Disadvantages:\n",
        "        Can be computationally expensive for large datasets, requires careful parameter tuning.\n",
        "# Addressing Class Imbalance:\n",
        "Oversampling:\n",
        "       Randomly duplicate minority class examples to create a more balanced dataset.\n",
        "# Undersampling:\n",
        "Randomly remove examples from the majority class.\n",
        "# Cost-sensitive learning:\n",
        "Assign higher weights to misclassifications of the minority class during model training.\n",
        "SMOTE (Synthetic Minority Over-sampling Technique):\n",
        "Generate synthetic instances of the minority class based on existing data points to address class imbalance.\n",
        "# Evaluation Metrics:\n",
        "# Accuracy:\n",
        "          Overall percentage of correct predictions, but not ideal for imbalanced datasets.\n",
        "# Precision:\n",
        "       Proportion of positive predictions that are actually positive (important for minimizing false positives, like sending legitimate emails to spam folder).\n",
        "# Recall:\n",
        "        Proportion of positive instances that are correctly classified (important for minimizing false negatives, like missing important emails).\n",
        "# F1-Score:\n",
        "      Harmonic mean of precision and recall, provides a balanced measure of performance in imbalanced datasets.\n",
        "\n",
        "# Busines\n",
        "# s Impact:\n",
        "Reduced Spam:\n",
        "            Implementing a spam filter effectively reduces the number of spam emails users receive, improving inbox usability and user satisfaction.\n",
        "# Increased Productivity:\n",
        "           Users spend less time sorting through spam, allowing them to focus on more productive tasks.\n",
        "# Improved Brand Reputation:\n",
        "       A reliable spam filter protects users from malicious content, enhancing the company's reputation.\n",
        "\n"
      ],
      "metadata": {
        "id": "kkEJe7j3Mp9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BfH6mjDJMqBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5PTlkhdMqGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}